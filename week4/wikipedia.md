# Wikipedia.pyのドキュメント

## 実行結果

### homework1

#### 方針

- 幅優先探索を用いて最短経路を探索する
- データの空間計算量を最小にするためpre_idという親ノードを格納するデータを用意し、探索するたびに{自分のid:親のid}というデータを格納する
- 最終的に目的のデータに辿り着いたら、pre_idを逆順に辿ることでpathを返すようにする

#### メンターさんにご指摘いただいた点

- 探索済みノードを格納するものをlistにしていたが、探索にO(n)かかってしまう　=> set()を用いればO(1)
- もともと自分の考えでは親のidのみ格納するのではなく、初期ノードからのpathを全て格納するようにしていたが、メモリの量などを考慮した方がいいことを教えていただいた

#### 渋谷からパレートの法則の実行結果(medium)

['渋谷', 'マクドナルド', 'Twitter', 'パレートの法則']

#### 渋谷からパレートの法則の実行結果(long)

['渋谷', 'マクドナルド', 'Twitter', 'パレートの法則']

#### 渋谷から小野妹子の実行結果(medium)

['渋谷', 'ギャルサー_(テレビドラマ)', '小野妹子']

### homework2

#### 方針

- それぞれのページランクを格納するデータを用意する(self.pagerank)
- ページランクを更新するときに格納するページランクバッファを用意する(self.pagerankbuf)
- この二つの値の差を計算し、全てのノードで収束しているかを判定する
  - 収束していたらページランクの順番にソートし、top10のtitleを返す
  - 収束していなければページランクの再計算を行う
- ページランクの計算の際にはrandom_surferアルゴリズムを採用し、振動してしまい収束しないことを防ぐ

#### メンターさんに教えていただいたこと

- もとの私のコードだと、random_surferアルゴリズムの時、接続ノードが存在しているノードの15%および孤立したノードの100%を計算した上で実際に全ノードに分配するということを実装していた
  - この考えだとO(N)*O(N)でO(N^2)になってしまう=>全てのノードに加えるランクは同じなので、変数に格納しておき、再分配を行う際に用いるようにすれば計算量はO(N)で実行できる！
- List.copy()という関数を用いればO(1)でコピーすることができると考えていたが、そうではなくO(N)かかってしまうということ
- デバッグする時にはwhile文の実行回数を記録するとわかりやすいということ


#### small(id,pagerankの順)

[{'C': 1.2230703124999995}, {'D': 1.2230703124999995}, {'B': 1.046673177083333}, {'E': 0.6617825520833331}, {'F': 0.6617825520833331}, {'A': 0.28362109374999994}]

#### medium(id.pagerankの順)

[{'英語': 1506.9345189453834}, {'ISBN': 959.4565658344422}, {'2006年': 525.999356897353}, {'2005年': 502.1544953035228}, {'2007年': 491.37822803701323}, {'東京都': 480.1650708705964}, {'昭和': 459.2634198524725}, {'2004年': 445.2579327101211}, {'2003年': 404.62199962466303}, {'2000年': 401.7729975736374}, {'2001年': 383.53994560530055}]

#### large(id.pagerankの順)

[{'英語': 4576.489503843239}, {'日本': 4569.188024981548}, {'VIAF_(識別子)': 3806.7042024638286}, {'バーチャル国際典拠ファイル': 3320.2497650737237}, {'アメリカ合衆国': 2714.42495422273}, {'ISBN': 2711.2834002133914}, {'ISNI_(識別子)': 2060.524273233074}, {'国際標準名称識別子': 1865.306608601629}, {'地理座標系': 1815.73497531216}, {'SUDOC_(識別子)': 1518.6286162872957}, {'イギリス': 1396.3751940051025}]


### homework3

#### 方針

- DFSの中でスタックに同じノードを複数回受け入れるコードを実装
- DFSを用いて1つパスを探索したら、先頭と末尾を除いた内側のパスを伸ばすことを考える
- 具体的には最初DFSにstartからgoalまでのパスを出力してもらい、そのパスの前から二番目と後ろから二番目をstartとgoalにし、DFSを回す、現在のpathよりも長ければ更新するということを繰り返す
- 現在の自分のpathの中で、確定部分に存在するノードには戻らないという制約を加える
- つまりDFSに渡す範囲になく、pathをして用いられているノードの集合を渡し、searched_setに加えることで実装する
- DFSを複数回探索することになるので、似た経路にならないよう、stackに格納する順番をランダムにする

#### メンターさんに教えていただいたこと

- もとの自分のコードの方針では、DFSのスタックに同じノードが複数回格納していいようにし、その都度pathを比較して長い方に更新するようにしていた
- そのコードだと計算量はO(N!)になってしまう(最大でノードNの並び方を全て考えることになってしまう)
- またsearched_setといった探索済みの集合を用意することができなかったため、無限ループに陥っていた
- そこで上記の考え方のヒントを教えていただきました

#### 実際に動かしてみて

- 重複がどうしても避けられていない。探索済み集合に入っていないもののループが許されてしまっているのか？今のコードでなぜ複数回同じノードが入ってしまうのか理解できていない

#### 実行結果(ランダム性が高いので長いと思ったものを保存)

#### small

1. ['F', 'C', 'A', 'B', 'D', 'E']

``` 
add longest path finished!!!
this is stack deque([(6, 1)])
this is stack deque([(3, 2)])
this is stack deque([(2, 3), (5, 3), (1, 3)])
this is stack deque([(2, 3), (5, 3), (2, 4)])
this is stack deque([(2, 3), (5, 3), (4, 5)])
this is stack deque([(2, 3), (5, 3), (5, 6)])
{'length': 6, 'path': ['F', 'C', 'A', 'B', 'D', 'E']}
add longest path finished!!!
this is stack deque([(3, 1)])
this is stack deque([(2, 2), (1, 2), (5, 2), (6, 2)])
this is stack deque([(2, 2), (1, 2), (5, 2)])
this is stack deque([(2, 2), (1, 2), (4, 3)])
{'length': 3, 'path': ['C', 'E', 'D']}
['F', 'C', 'A', 'B', 'D', 'E']
```

#### medium （重複が除けていなかったので間違っていました）

1. longest_path.txt(間違っています)
2. longest_path2.txt(間違っています)